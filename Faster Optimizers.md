# Faster Optimizers

While standard [[Gradient Descent]] works well, there are better models out there to optimize the [[Neural Network]], that provide a faster training time. Some of the most popular ones are:

- [[Momentum Optimization]]
- [[Nesterov Accelerated Gradient]]
- [[AdaGrad]]
- [[RMSProp]]
- [[Adam Optimization]]