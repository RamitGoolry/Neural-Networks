# ReLU Activation Function
__ReLU__ (Rectified Linear Unit) is an activation function commonly used in Deep Learning, to introduce a non-linearity into a [[Neural Network]], so that they can model complex data sets which can not be defined linearly.

A ReLU function has the formula: 
$$ \text{ReLU}(x) = max(0, x) $$

and it looks like : 
<center>
	<iframe src="https://www.desmos.com/calculator/xirvsdyac7?embed" width="500px" height="500px" style="border: 1px solid #ccc" frameborder=0></iframe>
</center>
The ReLU Activation functions suffers, from the [[Dying ReLU]] problem.